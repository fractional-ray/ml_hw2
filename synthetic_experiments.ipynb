{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "from linearclassifier import linear_predict, perceptron_update, plot_predictions, log_reg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "\n",
    "num_dim = 2\n",
    "num_points = 200\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'float'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-34b75658b402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of Examples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, normed, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3130\u001b[0m                       \u001b[0mhisttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhisttype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m                       \u001b[0mrwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3132\u001b[0;31m                       stacked=stacked, normed=normed, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   3133\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3134\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   6512\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6513\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6514\u001b[0;31m                     \u001b[0mxmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6515\u001b[0m                     \u001b[0mxmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6516\u001b[0m             \u001b[0mbin_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'float'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "# On the Mac we tested this on, using this random seed produces a fairly even class balance\n",
    "# Python is not always consistent across machines with preserving seeded random behavior, \n",
    "# so if your histogram shows major class imbalance, change this seed to get better balance\n",
    "\n",
    "data = np.random.randn(num_dim, num_points)\n",
    "true_model = {'weights': np.random.randn(num_dim, num_classes)}\n",
    "\n",
    "labels = linear_predict(data, true_model)\n",
    "plt.hist(labels)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create noisy labels\n",
    "noise_index = np.random.rand(num_points) < 0.3\n",
    "noisy_labels = labels.copy()\n",
    "noisy_labels[noise_index] = np.random.randint(0, num_classes, np.count_nonzero(noise_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot generated data\n",
    "markers = ['xr', 'ob', '*m',  'dk']\n",
    "\n",
    "plt.subplot(121)\n",
    "for i in range(num_classes):\n",
    "    plt.plot(data[0, labels == i], data[1, labels == i], markers[i])\n",
    "plt.title('Separable Data')\n",
    "\n",
    "plt.subplot(122)\n",
    "for i in range(num_classes):\n",
    "    plt.plot(data[0, noisy_labels == i], data[1, noisy_labels == i], markers[i])\n",
    "plt.title('Noisy Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into training and testing sets\n",
    "\n",
    "num_train = int(num_points / 2)\n",
    "num_test = num_points - num_train\n",
    "\n",
    "train_data = data[:, :num_train]\n",
    "test_data = data[:, num_train + 1:]\n",
    "\n",
    "# store noiseless and noisy labels in tuples\n",
    "\n",
    "train_labels = (labels[:num_train], noisy_labels[:num_train])\n",
    "test_labels = (labels[num_train + 1:], noisy_labels[num_train + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron experiment\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "train_accuracy = dict()\n",
    "test_accuracy = dict()\n",
    "model = dict()\n",
    "\n",
    "for is_noisy in (False, True):\n",
    "    model[is_noisy] = { 'weights': np.zeros((num_dim, num_classes)) }\n",
    "    train_accuracy[is_noisy] = np.zeros(epochs)\n",
    "    test_accuracy[is_noisy] = np.zeros(epochs)\n",
    " \n",
    "    for epoch in range(epochs):\n",
    "        # first measure training and testing accuracy            \n",
    "        predictions = linear_predict(train_data, model[is_noisy])\n",
    "        train_accuracy[is_noisy][epoch] = np.sum(predictions == train_labels[is_noisy]) / num_train\n",
    "\n",
    "        predictions = linear_predict(test_data, model[is_noisy])\n",
    "        test_accuracy[is_noisy][epoch] = np.sum(predictions == test_labels[is_noisy]) / num_test\n",
    "\n",
    "        # run perceptron training\n",
    "        mistakes = 0\n",
    "        for i in range(num_train):\n",
    "            correct = perceptron_update(train_data[:, i], model[is_noisy], train_labels[is_noisy][i])\n",
    "            \n",
    "            if not correct:\n",
    "                mistakes += 1\n",
    "        \n",
    "        print(\"Finished epoch %d with %d mistakes.\" % (epoch, mistakes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results of perceptron training\n",
    "\n",
    "plt.subplot(121)\n",
    "train_line = plt.plot(range(epochs), train_accuracy[False], label=\"Training\")\n",
    "test_line = plt.plot(range(epochs), test_accuracy[False], label=\"Testing\")\n",
    "plt.title('Separable Data')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "train_line = plt.plot(range(epochs), train_accuracy[True], label=\"Training\")\n",
    "test_line = plt.plot(range(epochs), test_accuracy[True], label=\"Testing\")\n",
    "plt.title('Noisy Data')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plot clean predictions\n",
    "\n",
    "test_predictions = linear_predict(test_data, model[False])\n",
    "train_predictions = linear_predict(train_data, model[False])\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_predictions(train_data, train_labels[False], train_predictions)\n",
    "plt.title('Training Data')\n",
    "plt.subplot(122)\n",
    "plot_predictions(test_data, test_labels[False], test_predictions)\n",
    "plt.title('Testing Data')\n",
    "\n",
    "print(\"Red markers indicate incorrect predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression gradient check\n",
    "\n",
    "# first check if the gradient and objective function are consistent with each other\n",
    "_ = log_reg_train(train_data, train_labels[True], {'lambda': 0.1}, \n",
    "              {'weights': np.random.randn(num_dim * num_classes)}, check_gradient=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "\n",
    "lambda_vals = 10 ** np.linspace(-9, 2, 12)\n",
    "\n",
    "train_accuracy = dict()\n",
    "test_accuracy = dict()\n",
    "model = dict()\n",
    "\n",
    "for is_noisy in (False, True):\n",
    "    model[is_noisy] = {'weights': np.zeros((num_dim, num_classes))}\n",
    "    \n",
    "    train_accuracy[is_noisy] = np.zeros(lambda_vals.size)\n",
    "    test_accuracy[is_noisy] = np.zeros(lambda_vals.size)\n",
    "    \n",
    "    for i in range(lambda_vals.size):\n",
    "        params = {'lambda': lambda_vals[i]}\n",
    "        \n",
    "        model[is_noisy] = log_reg_train(train_data, train_labels[is_noisy], params, model[is_noisy])\n",
    "        \n",
    "        train_predictions = linear_predict(train_data, model[is_noisy])\n",
    "        train_accuracy[is_noisy][i] = np.sum(train_predictions == train_labels[is_noisy]) / num_train\n",
    "\n",
    "        test_predictions = linear_predict(test_data, model[is_noisy])\n",
    "        test_accuracy[is_noisy][i] = np.sum(test_predictions == test_labels[is_noisy]) / num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results of logistic regression parameter sweep\n",
    "\n",
    "plt.subplot(121)\n",
    "train_line = plt.semilogx(lambda_vals, train_accuracy[False], label=\"Training\")\n",
    "test_line = plt.semilogx(lambda_vals, test_accuracy[False], label=\"Testing\")\n",
    "plt.title('Separable Data')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "train_line = plt.semilogx(lambda_vals, train_accuracy[True], label=\"Training\")\n",
    "test_line = plt.semilogx(lambda_vals, test_accuracy[True], label=\"Testing\")\n",
    "plt.title('Noisy Data')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plot clean predictions\n",
    "\n",
    "test_predictions = linear_predict(test_data, model[False])\n",
    "train_predictions = linear_predict(train_data, model[False])\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_predictions(train_data, train_labels[False], train_predictions)\n",
    "plt.title('Training Data')\n",
    "plt.subplot(122)\n",
    "plot_predictions(test_data, test_labels[False], test_predictions)\n",
    "plt.title('Testing Data')\n",
    "\n",
    "print(\"Red markers indicate incorrect predictions\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
